# starting from a base image of python
FROM python:3.9-slim

# create a working directory for the application
WORKDIR /app

# copy the requirements file to the working directory
COPY requirements.txt .
# copy the server script to the working directory
COPY server.py .

# install the required python packages
RUN pip install --no-cache-dir -r requirements.txt

# this is the port that the container uses
EXPOSE 8080

# command to run the server script
CMD ["python", "server.py"]

# Commands to build the docker image and run the container
# docker build . -f Dockerfile.infer -t infer_ml:v01
# docker run --detach -p 8080:8080 --rm -w /app/ -v /Users/ilsevandromme/project_docker/project_docker_microcredential/app:/app infer_ml:v01
# on https://localhost:8080/ i see "Welcome to Docker Lab."
# i tested the model inference with the following command:
#    curl -X POST http://localhost:8080/predict -H "Content-Type: application/json"  -d '{"input": [5.1, 3.5, 1.4, 0.2]}'
# and i received this response:
#    {"prediction": "setosa"}
#
#
# Because i received an error when building this image on HPC as apptainer image regarding incompatibility with linux vs arm64 silicon apple
# i rebuild the image with a platform option
# docker build --platform=linux/amd64 -t infer_ml:amd64 . -f Dockerfile.infer
# docker tag infer_ml:amd64 ilseclvd/infer_ml:v01-amd64
# docker push ilseclvd/infer_ml:v01-amd64

